{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "08 - Text 2 - Tokenization and Clustering",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMh0MhDP5lG0"
      },
      "source": [
        "# LEARNING GOALS\n",
        "#\n",
        "#                 - tokenization deeper dive\n",
        "#                 - reinforce text prep and tokenization options\n",
        "#                 - Cluster documents setup\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Em6KP7t66Vpn"
      },
      "source": [
        "# installs\n",
        "! pip install newspaper3k\n",
        "! pip install spacy\n",
        "! pip install nltk\n",
        "! pip install -U scikit-learn\n",
        "! pip install scikit-plot\n",
        "! pip install umap-learn\n",
        "! pip install tokenwiser"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHt-hLKZ6YJH"
      },
      "source": [
        "# imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import scikitplot as skplot\n",
        "\n",
        "import re\n",
        "\n",
        "# new imports\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer,TfidfVectorizer  \n",
        "import nltk\n",
        "from tokenwiser.textprep import HyphenTextPrep\n",
        "\n",
        "from newspaper import Article\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13t1J3a_CQpQ"
      },
      "source": [
        "############################################ Get some data from the using an awesome package Newspaper3k!\n",
        "## https://newspaper.readthedocs.io/en/latest/\n",
        "\n",
        "# Boston based chatbot company, now called Mainstay\n",
        "# URL = \"https://voicebot.ai/2021/02/16/conversational-ai-startup-admithub-raises-14m-for-higher-ed-chatbots/\"\n",
        "\n",
        "# # setup the article\n",
        "# article = Article(URL)\n",
        "\n",
        "# # get the page\n",
        "# article.download()\n",
        "\n",
        "# # parse it -- extracts all sorts of info\n",
        "# article.parse()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UaC-n-UGbHH"
      },
      "source": [
        "# what do we have -- b/c its for news sites, attempts to parse things like dates\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRu1IGHSCQsn"
      },
      "source": [
        "# the text -- what we are really after\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrTRgEAOEBE-"
      },
      "source": [
        "# tokenize\n",
        "# cv = CountVectorizer()\n",
        "\n",
        "# sklearn expects iterables, like lists\n",
        "# atokens = cv.fit_transform([atext])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzv1dM5BEqdc"
      },
      "source": [
        "# how many tokens --- note the new syntax of get feature names out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4FF55nuFnyo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDYGq6PcHM8u"
      },
      "source": [
        "# THOUGHT EXERCISE:\n",
        "# we have a doc-term  matrix with one doc and the terms in the columns\n",
        "# its effectively a 1d array\n",
        "# hypothetically, if we had a reference database with the same term representation (vocabulary)\n",
        "# what do you think we could do?\n",
        "\n",
        "# IDEAS:  identify similar records (scipy cdist) or scikitlearn nearest neighbors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAHlBSzPFoof"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9tUz8TeFvO5"
      },
      "source": [
        "################################################## Lets summarize\n",
        "##\n",
        "## we can use sklearn to keep things in our typical ml format\n",
        "## we can see that there is some pre-processing taking place\n",
        "## lets dive into that a bit more, and then discuss a flow using nltk -> sklearn\n",
        "\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
        "\n",
        "# - Notice the lower casing by default\n",
        "# - we can pass our own regex/tokenizer if we wanted, and some people do this (build their own)\n",
        "# - different ways to tokenize\n",
        "# - there are stopwords, but we can pass anything\n",
        "# - we can set the max number of tokens\n",
        "# - we can one hot encode = instead of counts, it can be 0/1 for the word/token\n",
        "# - we can create ngrams\n",
        "# - we can even validate the vocabulary if we wanted\n",
        "#\n",
        "# This last point brings up the concept of unseen words\n",
        "# Remember! sklearn fits the object, so any unseen words will not be parsed on new datsets with transform\n",
        "#\n",
        "# Summary: really powerful and adaptable, but means you plug in your own regex/tools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvXesJ93G-3S"
      },
      "source": [
        "################### part 1: - lets start with ngrams\n",
        "##\n",
        "## instead of single tokens, we can try to capture context by windowing the tokens/phrases\n",
        "## we can pass in a tuple of the ngrams, default is 1,1\n",
        "\n",
        "# a new dataset\n",
        "# corpus = [\"tokens, tokens everywhere\"]\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrtFud-R_fV5"
      },
      "source": [
        "# we could only have bigrams\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91NcQptP_O2x"
      },
      "source": [
        "# the key point is that you can imagine it might be able to retain context\n",
        "# if we combine tokens with other n-grams.  \n",
        "#\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdr2JnoGAjGW"
      },
      "source": [
        "###################################### Quick task\n",
        "## \n",
        "## build off the chatbot article from above\n",
        "## but instead of parsing the tokens (unigrams), include bigrams (2) and trigrams (3) \n",
        "## to the feature space\n",
        "##\n",
        "## how many features have we extracted from the article?\n",
        "## \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCovWMZ1KtnM"
      },
      "source": [
        "###################################### Question\n",
        "###### what does this say about our choice of tokenization\n",
        "###### what tools might help with this \"issue\"?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Jmwz8YzLDn9"
      },
      "source": [
        "###################################### Stopwords\n",
        "## by default stop words are not removed\n",
        "## there is a pre-built list of words, but let's ignore it\n",
        "## nltk is a great toolkit, but for now\n",
        "## lets just use the stopwords from that package\n",
        "\n",
        "# if this is your first time, you may need to download the stopwords\n",
        "# or on colab, for your session\n",
        "\n",
        "# nltk.download('stopwords')\n",
        "\n",
        "\n",
        "## OF COURSE, you could always downlod your own.  not the format of below, we just pass in a list in the end\n",
        "\n",
        "# lets get the stopwords\n",
        "# from nltk.corpus import stopwords\n",
        "# STOPWORDS = list(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPTEjJ9BLQ2_"
      },
      "source": [
        "# what do we have?\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSyzS47KNEcX"
      },
      "source": [
        "# the first few\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTbTa_p0NGpA"
      },
      "source": [
        "# note that everything is lower case!\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3DxF4LTNKUh"
      },
      "source": [
        "# admittedly this is harder to find than it should be\n",
        "# but the languages supported in NLTK\n",
        "\n",
        "# stopwords.fileids()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTlx-eqLN1x8"
      },
      "source": [
        "# now you can imagine that is pretty limiting above, I know\n",
        "# the other approach is to use spacy\n",
        "# https://spacy.io/usage/models\n",
        "# we will dive into spacy later, but I think its important to keep building the intuition\n",
        "# before going into model-driven work\n",
        "\n",
        "# last, we can always add to the stoplist if we wanted to now that its a list abvoe\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6p0N0yoOPFIT"
      },
      "source": [
        "# lets keep the corpus small, so use the original article\n",
        "# but remove stopwords\n",
        "\n",
        "\n",
        "\n",
        "# 281 -> 237\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWtMOgqDQN6z"
      },
      "source": [
        "# and of course, we can see the vocab\n",
        "# cv.vocabulary_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_kM4LexQ3iH"
      },
      "source": [
        "###################################### Max tokens\n",
        "## \n",
        "## this can be helpful if you want to restrict to the top N most frequent tokens\n",
        "## this restricts your space at the start\n",
        "## but the tradeoff is less common words, perhaps, could help with ML models\n",
        "##     -- the tokens/phrases are specific to he known label, and while rare, often occur for the label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNC6RrVdQ3mk"
      },
      "source": [
        "## we can use the article again, max with stopwords\n",
        "\n",
        "# cv = CountVectorizer(max_features=20, stop_words=STOPWORDS)\n",
        "# atokens = cv.fit_transform([atext])\n",
        "# cv.vocabulary_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_s-1bSY2Q3pp"
      },
      "source": [
        "###################################### character tokens\n",
        "## \n",
        "## if you wanted, you can parse characters\n",
        "## a little out of scope, but highlighting the concept of tokenization can \n",
        "## take all sorts of forms!\n",
        "\n",
        "# x = [\"Hello I can't\"]\n",
        "# charvec = CountVectorizer(analyzer='char', ngram_range=(1,1))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOQ0kaSWQ3ru"
      },
      "source": [
        "###################################### custom pattern\n",
        "## \n",
        "## if you really wanted to (or needed to), you can roll your own\n",
        "## tokenization\n",
        "## This is a little forward looking  .....\n",
        "## but highlights you all have the power to roll your own\n",
        "##\n",
        "## https://stackoverflow.com/questions/1576789/in-regex-what-does-w-mean\n",
        "##\n",
        "\n",
        "# alpha numeric plus a single quote/contraction\n",
        "# PATTERN = \"[\\w']+\"\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnrsSTtlWbBY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ue2kIPgnWeg-"
      },
      "source": [
        "###################################### Your Turn\n",
        "## \n",
        "## get the text from the two articles below using Newspaper3k\n",
        "## 1.  https://towardsdatascience.com/can-we-please-stop-using-word-clouds-eca2bbda7b9d\n",
        "## 2.  https://www.businessinsider.com/pie-charts-are-the-worst-2013-6\n",
        "##\n",
        "## create a bag of words representation of the two documents\n",
        "## keep the top 250 word tokens\n",
        "## remove stopwords baesd on the set we used above\n",
        "## use tokens, bigrams (2) and trigrams(3)\n",
        "## TRICKY!  Put back into a dataframe if you can\n",
        "## OPTIONAL:  Can you calculate the distance between the two docs?\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zL4NvOGjjzZh"
      },
      "source": [
        "## remember, jaccard is intersection over union, \n",
        "## instead of counts, we just said \"is this word present\"\n",
        "## value is proportion of elements that disagree\n",
        "\n",
        "## lets do a little more parsing before we start clustering!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YM_7uwWSfWaE"
      },
      "source": [
        "###################################### NLTK parsing\n",
        "###################################### Quick highlight that there are pre-built tools!\n",
        "## \n",
        "## we may not have to reinvent the wheel!\n",
        "## NLTK has some built in tooling we can leverage!\n",
        "## and trust me, other toolkits have their own approaches too!\n",
        "\n",
        "# from nltk.tokenize import word_tokenize, RegexpTokenizer, WordPunctTokenizer, TweetTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQOuKnJogvkd"
      },
      "source": [
        "# we may also need to download a tool to help with (sentence) parsing amongst other tasks\n",
        "# nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyDPDBJtfWb6"
      },
      "source": [
        "# corpus = ['I want my MTV! www.mtv.com', \"Can't I have it all for $5.00 @customerservice #help\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nxDJUJwhpiS"
      },
      "source": [
        "# want to zoom in on a tokenizer to help with twitter, and perhaps other social data\n",
        "# social = TweetTokenizer()\n",
        "\n",
        "# tokens_social = []\n",
        "# for doc in corpus:\n",
        "#   tokens_social.append(social.tokenize(doc))\n",
        "\n",
        "\n",
        "# # what do we have\n",
        "# tokens_social"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_N8xdZWibPz"
      },
      "source": [
        "###################################### Summary\n",
        "## \n",
        "## we have super powers via regex, but don't be afraid to look around\n",
        "## some decent tools in sklearn, but nltk has some custom utilities we can leverage\n",
        "##\n",
        "## We have options!  \n",
        "## we can try to parse with nltk and feed to sklearn\n",
        "## we can use the tooling in sklearn but might require we roll our own modifications\n",
        "##\n",
        "## but generally the flow is pre/tokenize -> bag of words of those tokens\n",
        "##"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kxno_Q1SxO6z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QY5Vgb5UwtjM"
      },
      "source": [
        "############################### So the big question\n",
        "## how does this all fit together?\n",
        "\n",
        "# build a function to pull in the bits we want from NLTK, or whatever framework we want to use\n",
        "# def tokenize(text):\n",
        "#   social = TweetTokenizer()\n",
        "#   tokens = social.tokenize(text)\n",
        "#   return tokens\n",
        "\n",
        "# # NOTE: lower case happens below, not above\n",
        "# combined = CountVectorizer(tokenizer=tokenize)\n",
        "# bow = combined.fit_transform(corpus)\n",
        "\n",
        "# bowdf = pd.DataFrame(bow.toarray(), columns=combined.get_feature_names())\n",
        "\n",
        "# bowdf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WV6AkgwRxwF2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMPmuDKFmt4V"
      },
      "source": [
        "###################################### Next Up:  Beyond simple counts with TFIDF\n",
        "##\n",
        "## instead of count vectors (which you can use, and should try, in your modeling!)\n",
        "## we can try to de-prioritize common words \n",
        "## This surfaces words that may be less common, but nuanced and we want to prioritize those tokens\n",
        "## "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fl5DA1o4z0ck"
      },
      "source": [
        "# the same data\n",
        "# corpus = [\"Can't I have it all for $5.00 @customerservice #help\", \n",
        "#           'I want my MTV!']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpirFnkVxM5n"
      },
      "source": [
        "# equivalent to CountVectorizer -> TfidfTransformer\n",
        "# basically if you want tfidf, do this, it saves a step\n",
        "# and you have the same options for parsing if you like\n",
        "\n",
        "# tfidf = TfidfVectorizer(token_pattern=\"[\\w']+\", ngram_range=(1,2))\n",
        "# tfidf.fit(corpus)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suzLPb717CJc"
      },
      "source": [
        "## just to call out, being able to specify the pattern can be \n",
        "## really powerful for specific tasks and business needs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6R_Tt0BxMzX"
      },
      "source": [
        "# lets put this into a dataframe\n",
        "# idf = tfidf.transform(corpus)\n",
        "\n",
        "# idf = pd.DataFrame(idf.toarray(), columns=tfidf.get_feature_names_out())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUME5JpYxMnc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-fChA66-F3n"
      },
      "source": [
        "# we could even heatmap this to help understand the intuition here\n",
        "\n",
        "# plt.figure(figsize=(4,6))\n",
        "# sns.heatmap(idf.T, xticklabels=True, yticklabels=True, cmap='Reds')\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wb8oMLOr726T"
      },
      "source": [
        "################### NOTE:\n",
        "## look at the weights generally, what do you see?\n",
        "## now focus in on the word in common, the token i\n",
        "##\n",
        "## we can see that when shared, there is the document effect\n",
        "\n",
        "# https://towardsdatascience.com/a-gentle-introduction-to-calculating-the-tf-idf-values-9e391f8a13e5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nqn1YoyuDNRg"
      },
      "source": [
        "\n",
        "## but why does this matter?\n",
        "##  We can think of tfidf as attempting to create a more informative feature space\n",
        "##  when we think about similiarty, or how we could reduce this space easily, \n",
        "##  its not hard to consider that DR techniques give us ways to compress but \n",
        "##  we lose the ability to describe the impact of a given token.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXBLwDnzL380"
      },
      "source": [
        "# lets put this back onto the SMS dataset\n",
        "\n",
        "# sms = pd.read_gbq(\"SELECT label, message from `questrom.SMSspam.train`\", \"questrom\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1ehwItLMOlB"
      },
      "source": [
        "# what do we have again\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7u5rbbqBMVjC"
      },
      "source": [
        "# get this into a doc term representation\n",
        "\n",
        "# tfidf = TfidfVectorizer(max_features=1000, ngram_range=(1,3))\n",
        "# sms_idf = tfidf.fit_transform(sms.message)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-OOKfhxMwwS"
      },
      "source": [
        "# what do we have\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x03DNd6fzgKq"
      },
      "source": [
        "# compress the dimensions with UMAP\n",
        "\n",
        "# from umap import UMAP\n",
        "\n",
        "# umap = UMAP(2)\n",
        "# u2 = umap.fit_transform(sms_idf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7kNkRx7GY_k"
      },
      "source": [
        "# we just compressed the tfidf array to 2d\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xf1EURa0NAxO"
      },
      "source": [
        "# lets plot this up\n",
        "\n",
        "# u2df = pd.DataFrame(u2, columns=[\"e1\", \"e2\"])\n",
        "# sns.scatterplot(data=u2df, x=\"e1\", y=\"e2\", hue=sms.label, alpha=.5) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jC60NbXvO42O"
      },
      "source": [
        "# we could even attempt to cluster from here\n",
        "\n",
        "# from sklearn.cluster import KMeans\n",
        "\n",
        "# ss = []\n",
        "\n",
        "# for i in range(2, 10):\n",
        "#   km = KMeans(i)\n",
        "#   km.fit(u2)\n",
        "#   ss.append(km.inertia_)\n",
        "\n",
        "# sns.lineplot(range(2,10), ss)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBcE1FaGPbeh"
      },
      "source": [
        "###################################### whats next?\n",
        "##\n",
        "## we will build on clustering as we go -- we can use what we saw in the first half, and the \n",
        "## vocabulary to help reason about how clusters might help us with themeing\n",
        "## sentiment analysis- the easy vs the good (in my opinion, of course)\n",
        "## review how/why this can work, and why sentiment is easy to do poorly\n",
        "## continue to see how text and machine learning fit very well together\n",
        "## spacy to get us thinking about parsing the entities, and gensim preview\n",
        "## "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDJdLlHg1qKF"
      },
      "source": [
        "###################################### BREAKOUT Challenge\n",
        "##\n",
        "## Get the topics from big query\n",
        "## questrom:datasets.topics\n",
        "## parse the text into bag of words\n",
        "## (only the text, not the category) - your choice on tokenization and weighting/feature space\n",
        "## cluster the text\n",
        "## how many clusters do you have?\n",
        "## overlay the category on top of the clusters\n",
        "## if we didn't have the category, any evidence that  text processing and clustering would help\n",
        "## find patterns?  Are there documents that appear to be outliers?\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vF9CxtJipF8A"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmchdVzMpGWw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}